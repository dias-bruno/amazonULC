---
title: "Feature Preparation and Selection and Supervised Land Cover Classification"
author: "BDS and AP"
format: html
jupyter: python3
execute: 
  cache: true
---

## Introduction
In this Notebook, we present a methodology to treat and select variables and subsequent supervised classification of a land cover base by the Random Forest algorithm. Given a shapefile (.shp) containing a variable with sample classes, the attributes will be treated by removing missing values, normalized and ordered about their explanatory power from the calculation of the R² of ANOVA. Subsequently, the attributes will be compared two by two, and if the correlation (of Pearson) exceeds a desired threshold, the variable with the lowest explanatory power will be removed. Subsequently, the best possible model generated by Random Forest will be tested, given a set of hyperparameters. The hyperparameters are chosen using the RandomizedSearch technique, adopting a K-Fold with five groups and measuring the performance of each combination with the F1-Score. As output, a shapefile with the treated segments and another with the final classification are exported.

## Quarto
Load python libraries used in the notebook:
```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import math
import folium
import mapclassify
import geopandas as gpd
import shapely
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score,ConfusionMatrixDisplay,classification_report,confusion_matrix
```

Reading the segments with features and creating a GeoDataFrame. We generate the segments after a segmentation of the WPM image (CBERS-4A), using the meanshift algorithm. The segments have variables with spectral and textural information extracted from a WPM image from the CBERS-4A satellite, as well as geometric information (such as m² area and perimeter):
```{python}
obj = gpd.read_file("raw-images\\nivel1_maraba.shp")
obj
```
Defining the identifier variable of each segment:
```{python}
indice = 'ID'
```

Defining the variable with the sample classes. This variable must be categorical or discrete to perform the classification. We defined the classes: “Shrub Vegetation” (VA), “Herbaceous Vegetation” (VH), “Water” (AG), “Exposed Ground” (SE), “High Gloss Cover” (BR), “Ceramic Cover” (TC), “Fiber Cement Cover” (TF), “Asphalt Road” (AS), “Terrain Road” (ES), “Cloud” (NU) and “Shadow” (SO):
```{python}
TARGET = 'TARGET'

target = []
cont = 0

for classe in obj[TARGET].unique():
    if not pd.isnull(classe):
        target.append(classe)

target
```

Copying the 'index' and 'geometry' information from the original base to a new GeoDataFrame (geom):
```{python}
geom = obj[[indice,'geometry']]
geom
```

Extracting categorical (var_cat) and numerical variables (var_num) for the data treatment:
```{python}
var_num = pd.DataFrame(obj.select_dtypes(include=['float64','int64','int','float']))
var_cat = pd.DataFrame(obj.select_dtypes(include=['string','object']))
var_cat
var_num
```
Removing the sample column for the data treatment:
```{python}
try: 
    var_cat = var_cat.drop(columns = TARGET)
except:
    var_num = var_num.drop(columns = TARGET)
```

Removing outliers from numeric fields:
```{python}
#Function for removing outliers by considering as outlier values greater than 2.698 σ (standard deviation) of the normal distribution curve: 

def rmv_outliers(DataFrame, col_name):
    intervalo = 2.698 * DataFrame[col_name].std()
    media = DataFrame[col_name].mean()
    DataFrame.loc[DataFrame[col_name] < (media - intervalo), col_name] = np.nan
    DataFrame.loc[DataFrame[col_name] > (media + intervalo), col_name] = np.nan

for coluna in var_num.columns:
    rmv_outliers(var_num, coluna)
```

Normalizing and filling empty values in numeric fields:
```{python}
#dummy = var_num.iloc[:,1:].mean()
dummy = 0

var_num.iloc[:,1:] = var_num.iloc[:,1:].fillna(dummy)

var_num.iloc[:,1:] =(var_num.iloc[:,1:] - var_num.iloc[:,1:].min())/(var_num.iloc[:,1:].max() - var_num.iloc[:,1:].min())
var_num
```

Applying OneHotEncoder to variables of a categorical type and creating a DataFrame of the data:
```{python}
aux = obj[[indice, TARGET]]

try:
    var_cat = pd.get_dummies(var_cat[:-1], drop_first=True)
    obj = (aux.merge(var_num, left_on=indice, right_on=indice)).merge(var_cat, left_index=True, right_index=True)
    
except:
    obj = (aux.merge(var_num, left_on=indice, right_on=indice))
    print("Não há variáveis categóricas para aplicar OneHotEncode")

obj
```

Viewing descriptive statistics of the already processed database:
```{python}
obj.describe()
```

Creating a copy of the already treated DataFrame:
```{python}
saida =  obj
saida
```

Selecting only the sampled features to extract the statistics:
```{python}
obj = obj[obj[TARGET].isin(target)]
obj
```

Calculate the $R^2$ of the Anova of each column against the Target column:
```{python}
iv = {}

for coluna in obj.columns:  
    if coluna != TARGET:
        counts = obj.groupby(TARGET, sort=True)[coluna].count() # Count of elements in each sample class
        medias = obj.groupby(TARGET, sort=True)[coluna].mean() # Average of each caluna per sample class 
        aux = 0        
        for i in range(len(counts)):
            try:
                aux = aux + counts.iloc[i]*((medias.iloc[i] - obj[coluna].mean())**2)
            except:
                aux = 0
        
        if (sum(counts))*((obj[coluna].std())**2) == 0:
            iv[coluna] = aux/0.00001
        else:                
            iv[coluna] = aux/((sum(counts))*((obj[coluna].std())**2))
        
        print("R² computed for ", coluna)

iv = sorted(iv.items(), key=lambda x: x[1], reverse=True)
iv
```

Bloxpot plot of the variable with the best explaining power in relation to the sample classes:
```{python}
plt.figure(figsize=(15,5))
sns.boxplot(x=TARGET , y= obj[iv[0][0]], order= obj[TARGET].sort_values().unique(), data = obj)
```

Bloxpot plot of the variable with the worst explaining power in relation to the sample classes:
```{python}
plt.figure(figsize=(15,5))
sns.boxplot(x=TARGET , y= obj[iv[-1][0]], order= obj[TARGET].sort_values().unique(), data = obj)
```

Removing poorly explanatory variables from a threshold (lim_min) and variables with possible 'NaN' values:
```{python}
lim_min = 0.1
aux = []

print(f'Total variables before removal: {len(iv)}')

for i in range(len(iv)):
    if math.isnan(iv[i][1]):
        aux.append(iv[i])
    elif iv[i][0] != indice:
        if iv[i][1] < lim_min:
            aux.append(iv[i])

for i in aux:
    iv.remove(i)

print(f'Total variables after removal: {len(iv)}')
```

Viewing the most explanatory variables:
```{python}
iv
```

Defining the correlation factor to be considered.

Given two variables [i,j], the correlation between i and j will be calculated. If the correlation between the two variables is greater than the maximum correlation factor, we will exclude the one with less explanatory power from its $R^2$:
```{python}
factor = 0.70
```

Removal of highly correlated (Pearson) variables:
```{python}
colunas = []
aux = []

for i, j in iv:
    colunas.append(i)
    aux.append(i)

for i in range(len(colunas)):
    for j in range(len(colunas)):
        if j > i and abs(saida[colunas[i]].corr(saida[colunas[j]])) > factor:
            if (colunas[j] in aux) and (colunas[j] != indice):
                aux.remove(colunas[j])

aux = sorted(aux)
aux.remove(indice)
aux[:0] = [indice]
aux
```

Output Dataframe variables:
```{python}
print(f'Total variables after second removal process: {len(aux)}')
print(f'list of variables remaining after the variable selection process: {aux}')
```

Visualizing the correlation between variables:
```{python}
corr_df = saida[aux].corr()

corr_df.style.background_gradient(cmap='Spectral')
corr_df = saida[aux[1:]].corr(method='pearson')

plt.figure(figsize=(10, 10))
sns.heatmap(corr_df, annot=False)
plt.show()
```

```{python}
aux.append(TARGET)
```

Saving a shapefile with the treated data:
```{python}
geom = geom.merge(saida[aux], left_on=indice, right_on=indice)
geom
geom.to_file("data-outputs\\maraba_segments_treated.shp")
```

##Supervision Classification of the land cover

Selecting only the database with samples to build the supervised classification model:
```{python}
amostras = geom.replace(to_replace='None', value=np.nan).dropna()
amostras
```

Separating the training (x_train, y_train) and test (x_test, y_test)data:
```{python}
X = pd.DataFrame(amostras.iloc[:,2:-1])
Y = pd.DataFrame(amostras[TARGET]).to_numpy()

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify = Y, random_state=42)
```

Visualizing the shape of the training and test data:
```{python}
x_train.shape
y_train.shape
x_test.shape
y_test.shape
```

Visualizing the sample classes and the count of features per class:
```{python}
amostras[TARGET].unique()
amostras.groupby(TARGET)[indice].nunique()
```

Selection of hyperparameters and values for RandomizedSearchCV. In this technique, we input the algorithm values of hyperparameters that will be randomly selected and combined, returning a combination that results in the best possible classification. The algorithm uses a performance metric and a predetermined number of iterations to choose the best combination of hyperparameters. Randomized Search uses cross-validation, dividing the training base into k parts (folds), and the model is trained and evaluated k times. For each iteration, the algorithm selects a part (fold) that will serve as an evaluation and trains the model on the other k -1 parts:
```{python}
parametros = {'n_estimators':[1,20,50,100,150,200,250,300,350,400,450,500,550,600,700,800,900,1000,1500,2000],
              'criterion':['gini','entropy'],
              'max_depth':[5,10,20, None],
              'min_samples_split':[2,5,10],
              'min_samples_leaf': [1, 2, 4],
              'bootstrap': [True, False]}
```

Defining the RandomizedSearchCV (with K-fold = 5):
```{python}
modelo = RandomizedSearchCV(estimator = RandomForestClassifier(), n_iter = 100, verbose=2, random_state=42, param_distributions = parametros, scoring='f1_macro', n_jobs= None, cv=5)
```

Designing the Random Forest classification model:
```{python}
modelo.fit(x_train, y_train.ravel())
```

Visualizing the best combination of hyperparameters:
```{python}
modelo.best_params_
modelo.best_score_
modelo.best_estimator_
```

Using the trained model to predict the validation database:
```{python}
y_pred = modelo.predict(x_test)
```

Viewing the most used variables in the Random Forest model:
```{python}
plt.barh(x_test.columns, modelo.best_estimator_.feature_importances_ )
```

Viewing performance metrics of the classification model:
```{python}
macro = f1_score(y_test, y_pred, average = 'macro')
wei = f1_score(y_test, y_pred, average = 'weighted')
accuracy = accuracy_score(y_test, y_pred)

results = {'F1_Score_Macro': macro,
            'F1_Score_Weighted': wei,
            'Global Acuraccy': accuracy 
            }

pd.DataFrame.from_dict(results, orient='index', dtype=None, columns=['Métricas'])
```

Viewing the Confusion matrix for the test data:
```{python}
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
data = {'Reference': y_test.flatten(), 'Predicted': y_pred}
df = pd.DataFrame(data, columns = ['Reference','Predicted'])
mc = pd.crosstab(df['Reference'], df['Predicted'], rownames=['Reference'], colnames=['Predicted'])
plt.figure(figsize=(10,10))
sns.heatmap(mc, annot=True,fmt='g',  cmap = 'YlGnBu',linewidths=.5)
plt.title('Confusion matrix', fontweight='bold', fontsize=14)
plt.xlabel('Classification', fontsize=12)
plt.ylabel('Reference',fontsize=12)
plt.show()
classe = []
Accuracy = []
Precision = []
Recall = []
F1_Score = []

for i in range(mc.shape[0]):
    TP = mc.iloc[i,i]
    FP = mc.iloc[i,:].sum() - TP
    FN = mc.iloc[:,i].sum() - TP
    TN = mc.sum().sum()-TP-FP-FN
    
    classe.append(mc.index[i]) 
    Accuracy.append((TP+TN)/mc.sum().sum())
    Precision.append(TP/(TP+FP))
    Recall.append(TP/(TP+FN))
    F1_Score.append(((2*Precision[i]*Recall[i])/(Precision[i] + Recall[i])))
    

avaliacao = {'classe': classe,
            'Precision': Precision,
             'Recall': Recall,
             'F1_Score':F1_Score,
             'Acuraccy':Accuracy
            }
       
pd.DataFrame(avaliacao)
```

Inserting the classification ('CLASS') into the database shapefile:
```{python}
geom['CLASS'] = modelo.predict(geom.iloc[:,2:-1])
```

Viewing the Land cover classification map:
```{python}
geom.plot(column ='CLASS', legend=True, cmap = 'tab20', categorical=True, legend_kwds={'loc': 'center left', 'bbox_to_anchor':(1,0.5)})
geom.explore(column="CLASS", tooltip="CLASS",tiles="CartoDB positron",
             categorical = True, cmap='Set2', style_kwds=dict(color="grey", weight=0.01))
```

Exporting the shapefile with classification:
```{python}
geom.to_file("data-outputs\\maraba_land_cover.shp")
```
